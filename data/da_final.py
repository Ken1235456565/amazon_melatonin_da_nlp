# -*- coding: utf-8 -*-
"""DA-final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NGElJdat7DZHfZBYzFqXRlpVUyyNdrVR

# DAMG6105 Final Project

### Kaining Tian

### OverView

problem define: notes and questions

Melatonin supplements come in a wide range of doses (1mg, 3mg, 5mg, 10mg, 12mg, etc.) and product types (gummies, tablets, time-release), making the consumer market highly chaotic.
This project analyzes five different melatonin products scraped from Amazon to understand:

1. Do different dosages affect market feedback?
2. Is the feedback influenced more by other factors?
3. What do people expect for each dosage?
4. What are they praise\complain about for each dosage?

#### Reading the dataset
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import seaborn as sns
import matplotlib.pyplot as plt

raw_df1 = pd.read_csv("melatonin-amazon-1mg.csv",encoding = "ISO-8859-1")
raw_df2 = pd.read_csv("melatonin-amazon-3mg.csv",encoding = "ISO-8859-1")
raw_df3 = pd.read_csv("melatonin-amazon-5mg.csv",encoding = "ISO-8859-1")
raw_df4 = pd.read_csv("melatonin-amazon-10mg.csv",encoding = "ISO-8859-1")
raw_df5 = pd.read_csv("melatonin-amazon-12mg.csv",encoding = "ISO-8859-1")
raw_df6 = pd.read_csv("melatonin-amazon-20mg.csv",encoding = "ISO-8859-1")

raw_df = pd.concat([raw_df1, raw_df2, raw_df3, raw_df4, raw_df5, raw_df6],ignore_index=True)

"""#### Explore the dataset"""

raw_df.head(5)

raw_df.columns

raw_df.shape

raw_df.size

raw_df.info()

raw_df.isnull().sum().sum()

raw_df['ASIN'].nunique()

"""### Data Cleaning

#### Title Parsing
"""

import re

"""Dosage"""

raw_df["Dosage_mg"] = raw_df["ProductTitle"].str.extract(
    r'(?<!\d)(\d+)\s*mg(?!/)', expand=False
).astype(float)

"""Form"""

raw_df["Form"] = raw_df["ProductTitle"].str.lower().str.extract(
    r'(gummy|gummies|fast dissolve|quick dissolve|tablets?|capsules?)',
    expand=False
)

"""Flavor"""

raw_df["Flavor"] = raw_df["ProductTitle"].str.extract(
    r'(strawberry|mixed berry|natural berry|blackberry|berry|lemon|orange|citrus|cherry|blueberry)',
    flags=re.IGNORECASE,
    expand=False
)

"""Count"""

p1 = raw_df["ProductTitle"].str.extract(
    r'(\d+)\s*(?:count|tablets?)',
    flags=re.IGNORECASE
)

p2 = raw_df["ProductTitle"].str.extract(
    r'(\d+)\s*(?:\w+\s+)*tablets',
    flags=re.IGNORECASE
)

p3 = raw_df["ProductTitle"].str.extract(
    r'(\d+)\s*day supply',
    flags=re.IGNORECASE
)

raw_df["Count"] = p1.fillna(p2).fillna(p3).astype(float)

"""#### check data type"""

raw_df["HelpfulCounts"].dtype

raw_df["HelpfulCounts"] = pd.to_numeric(raw_df["HelpfulCounts"], errors="coerce")

raw_df["ReviewDate"].dtype

raw_df['ReviewDate'] = pd.to_datetime(raw_df['ReviewDate'], errors='coerce')

"""#### Missing values"""

raw_df.isnull().sum()

"""need deal with HelpfulCounts, Images, ReviewContent, ReviewTitle, Reviewer

HelpfulCounts fill 0, means no one comment
"""

raw_df['HelpfulCounts'] = raw_df['HelpfulCounts'].fillna(0)

"""Images not used so drop"""

raw_df = raw_df.drop(columns=['Images'])

"""dop the ones with no comment, because this is core data"""

raw_df = raw_df[raw_df['ReviewContent'].notna()]

"""not important, fill with empty string"""

raw_df['ReviewTitle'] = raw_df['ReviewTitle'].fillna('')

raw_df['Reviewer'] = raw_df['Reviewer'].fillna('')

raw_df.isnull().sum()

"""#### Data Reduction"""

df_clean = raw_df[raw_df['Verified'] == True].copy()
df_clean.drop(["PageUrl","ParentId","ProductLink","ProductTitle","Reviewer","Verified"],axis=1,inplace=True)

df_clean.head()

"""### Analyzing the data

### 2.3 Univariate analysis

quantitive data: HelpfulCounts/ReviewScore/Dosage_mg/Count
"""

df_clean.describe()



"""conclusions:
1. HelpfulCounts(std, max-min, 75%) --> data have extreme big values, A few comments attract a lot of attention, while the majority of comments receive no attention.
2. ReviewScore(50%, mean) --> The median (4) is high, and the mean (3.3) is also high.
3. Dosage(mean, 50%) --> mediam dosage have more comments
4. Count(Mean, 50%, 75%) --> bigger package have more comments

visulization provement

HelpfulCounts
"""

plt.figure(figsize=(8,4))
sns.boxplot(x=df_clean['HelpfulCounts'])
plt.title("Boxplot")
plt.show()

"""ReviewScore"""

plt.figure(figsize=(8,4))
sns.boxplot(x=df_clean['ReviewScore'])
plt.title("Boxplot")
plt.show()

"""Dosage"""

plt.figure(figsize=(8,4))
sns.countplot(x=df_clean['Dosage_mg'])
plt.title("DosageDiagram")
plt.show()

"""count"""

plt.figure(figsize=(8,4))
sns.countplot(x=df_clean['Count'])
plt.title("countdiagram")
plt.show()

"""Categorical data: Brand / Form / Flavor"""

plt.figure(figsize=(8,4))
sns.countplot(x='Brand', data=df_clean)

plt.figure(figsize=(8,4))
sns.countplot(x='Form', data=df_clean)

plt.figure(figsize=(8,4))
sns.countplot(x='Flavor', data=df_clean)

"""medium dosage\big package\dissolve form have more reviews

### 2.4 Bivariate analysis

Dosage vs ReviewScore
"""

plt.figure(figsize=(10,5))
sns.boxplot(x='Dosage_mg', y='ReviewScore', data=df_clean)
plt.show()

sns.barplot(x='Dosage_mg', y='ReviewScore', data=df_clean)

"""ReviewScore vs Count"""

plt.figure(figsize=(10,5))
sns.boxplot(x='Count', y='ReviewScore', data=df_clean)
plt.show()

sns.barplot(x='Count', y='ReviewScore', data=df_clean)

"""ReviewScore vs Form"""

plt.figure(figsize=(10,5))
sns.boxplot(x='ReviewScore', y='Form', data=df_clean)
plt.show()

sns.barplot(x='Form', y='ReviewScore', data=df_clean)

"""ReviewScore vs Brand"""

plt.figure(figsize=(12,6))
sns.boxplot(x='ReviewScore', y='Brand', data=df_clean)
plt.show()

sns.barplot(x='Brand', y='ReviewScore', data=df_clean)

"""ReviewScore vs Flavor"""

plt.figure(figsize=(12,6))
sns.boxplot(x='ReviewScore', y='Flavor', data=df_clean)
plt.show()

sns.barplot(x='Flavor', y='ReviewScore', data=df_clean)

"""low_score_ratio

1. ReviewScore is not strongly affected by any of these factors
3. medium dosage have the lowest reviewScore.
4. medium package(count) have the lowest reviewScore.
5. flavor brand and form don't have big difference

### 2.5 Multivariate analysis
"""

import scipy.stats as ss

# Cramér's V function
def cramers_v(x, y):
    table = pd.crosstab(x, y)
    chi2 = ss.chi2_contingency(table)[0]
    n = table.sum().sum()
    r, k = table.shape
    return np.sqrt(chi2 / (n * (min(r, k)-1)))

# Variables you want to test
cols = ['ReviewScore','Dosage_mg','Count',"Flavor","Brand","Form"]

# Build the Cramér matrix
cramer_matrix = pd.DataFrame(np.zeros((len(cols), len(cols))),
                             index=cols, columns=cols)

for i in range(len(cols)):
    for j in range(len(cols)):
        x = df_clean[cols[i]]
        y = df_clean[cols[j]]
        # For numerical variables, discretize or treat as categories
        if x.dtype != 'object' and not pd.api.types.is_categorical_dtype(x):
            x = pd.cut(x, bins=5)  # discretize continuous vars
        if y.dtype != 'object' and not pd.api.types.is_categorical_dtype(y):
            y = pd.cut(y, bins=5)
        cramer_matrix.iloc[i, j] = cramers_v(x, y)

# Plot heatmap
plt.figure(figsize=(10,7))
sns.heatmap(cramer_matrix, annot=True, cmap="Blues", vmin=0, vmax=1)
plt.title("Cramér's V Heatmap for Categorical Variables")
plt.show()

sns.pairplot(df_clean[['ReviewScore','Dosage_mg','Count',"Flavor","Brand","Form"]], diag_kind='kde')

"""1. ReviewScore is not very strongly relevant to any of the factors.
2. Form and brand have a strong influence on flavor.

conclusion:
1. ReviewScore is related to not only truth but also expectation, which can be different(eg.people buy low dose product expact the effect to be slight)
2. Is ReviewScore influenced by other people's opinion(high HelpfulCounts review)?
3. all the ReviewScore is relatively high, probably part of the advertisement plan
"""

df_clean[df_clean['HelpfulCounts'] > 5]['ReviewScore'].value_counts(normalize=True)

df_clean[df_clean['HelpfulCounts'] <= 5]['ReviewScore'].value_counts(normalize=True)

"""high HelpfulCounts review have high ReviewScore
1. Helpful > 5: Higher percentage of high scores
2. Helpful ≤ 5: Significantly higher percentage of low-score ratings
"""

# Get normalized score distribution
score_dist = df_clean[df_clean['HelpfulCounts'] >= 5]['ReviewScore'].value_counts(normalize=True)

score_dist = score_dist.sort_index()

plt.figure(figsize=(8,5))
sns.barplot(x=score_dist.index, y=score_dist.values, palette="Blues_d")

plt.title("ReviewScore Distribution (HelpfulCounts >= 5)", fontsize=14)
plt.xlabel("ReviewScore")
plt.ylabel("Proportion")

for i, v in enumerate(score_dist.values):
    plt.text(i, v + 0.01, f"{v:.2f}", ha='center', fontsize=12)

plt.ylim(0, max(score_dist.values) + 0.05)
plt.show()

df_prod = df_clean.groupby('ASIN').agg(
    avg_score=('ReviewScore', 'mean'),
    median_score=('ReviewScore', 'median'),
    q25_score=('ReviewScore', lambda x: x.quantile(0.25)),
    q75_score=('ReviewScore', lambda x: x.quantile(0.75)),
    std_score=('ReviewScore', 'std'),

    high_helpful_count=('HelpfulCounts', lambda x: (x > 5).sum()),
    max_helpful=('HelpfulCounts', 'max'),
    total_reviews=('ReviewScore', 'count')
).reset_index()

sns.scatterplot(x='high_helpful_count', y='q25_score', data=df_prod)
plt.xlabel("Number of helpful reviews (>5)")
plt.ylabel("Average product ReviewScore")
plt.title("High Helpful Reviews vs Average Rating")
plt.show()

"""There is almost no correlation between a high number of helpful reviews and the average product rating."""

df_prod['max_helpful'] = df_prod['max_helpful']

sns.scatterplot(x='max_helpful', y='q25_score', data=df_prod)
plt.xlabel("Max helpful")
plt.ylabel("Average ReviewScore")
plt.title("Is the product dominated by a single super-helpful review?")
plt.show()

"""A single "viral comment" will not affect everyone's rating attitude."""

df_clean.groupby("ASIN")["ReviewScore"].describe()

"""find that all products share virtually identical score structures:
ReviewScore does not meaningfully differentiate products

conclusion for nlp:
1. Product review is not strongly affected by product attributes or even advertisement.
2. All products have both positive and negative reviews, but the overall rating is high.
3. Each product has a significant number of negative experiences
4. but these negative experiences don't lower the average rating (because there are too many 4-5 star reviews).
5. The rating reflects the user's subjective experience, expectations, current emotions, and individual differences.

next step:
1. find different expectations for different dosage
2. separate review with score and see how the expectations meet\ missed
3. find out whether the high HelpfulCounts review influence the whole review?

### NLP

#### 3.1Normalization
"""

comment = df_clean.copy()

import re

def clean_text(text):
    text = re.sub(r'@[A-Z_a-z_0-9_]+', '', text)
    text = re.sub(r'https?:\/\/\S+', '', text)
    text = re.sub(r'#', '', text)
    text = text.lower()
    return text

comment['ReviewContent'] = comment['ReviewContent'].apply(clean_text)

def allclean(text):
  text = re.sub('[^A-Za-z0-9]', ' ', text)
  return(text)

comment['ReviewContent'] = comment['ReviewContent'].apply(allclean)

comment['ReviewContent']

raw_freq = pd.Series(' '.join(comment['ReviewContent']).split()).value_counts()
raw_freq



"""#### Stop Words Removal"""

stop = pd.read_csv('stop_w.txt', encoding = "ISO-8859-1")
stop.head()

stop = stop['stops'].astype(str).str.strip().tolist()

comment['ReviewContent'] = comment['ReviewContent'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))

comment['ReviewContent']

freq_cleaned = pd.Series(' '.join(comment['ReviewContent']).split()).value_counts()[:20]
freq_cleaned

comment.head()



"""#### Tokenization, Counting and Normalizing the tokens"""

from sklearn.feature_extraction.text import CountVectorizer

cvect = CountVectorizer(min_df = 1, max_df = 0.9)
X = cvect.fit_transform(comment['ReviewContent'])

word_freq_df = pd.DataFrame({'term': cvect.get_feature_names_out(),
                             'occurrences':np.asarray(X.sum(axis=0)).ravel().tolist()})

word_freq_df['frequency'] = word_freq_df['occurrences']/np.sum(word_freq_df['occurrences'])

word_freq_df

word_freq_df.sort_values('occurrences', ascending = False).head(20)

fig = plt.figure(figsize = (10, 5))
plot=sns.barplot(x="term",y="frequency", data= word_freq_df.sort_values('frequency', ascending = False)[0:20])
plot.set_xticklabels(rotation=75, labels = word_freq_df.sort_values('frequency', ascending=False).term[0:20])
plt.title("Most frequently used words in review", y = 1.07, fontsize = 17);



from sklearn.feature_extraction.text import TfidfVectorizer

tvect = TfidfVectorizer(stop_words='english', max_features= 1000, max_df = 0.5, smooth_idf=True)
doc_vec = tvect.fit_transform(comment['ReviewContent'])
names_features = tvect.get_feature_names_out()

dense = doc_vec.todense()
denselist = dense.tolist()
df = pd.DataFrame(denselist, columns = names_features)



"""#### N-grams"""

#Bi-gram
def top_n2_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(2,2),
            max_features=2000).fit(corpus)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in
                  vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1],
                reverse=True)
    return words_freq[:n]

# n2
top2_words = top_n2_words(comment['ReviewContent'], n=200)
top2_df = pd.DataFrame(top2_words)
top2_df.columns=["Bi-gram", "Freq"]
top2_df

top20_bi_gram = top2_df.iloc[0:20,:]
fig = plt.figure(figsize = (10, 5))
plot=sns.barplot(x=top20_bi_gram["Bi-gram"],y=top20_bi_gram["Freq"])
plot.set_xticklabels(rotation=90,labels = top20_bi_gram["Bi-gram"])
plt.title("Most two words used together in review", y=1.05, fontsize=17);



def top_n3_words(corpus, n=None):
    vec1 = CountVectorizer(ngram_range=(3,3),
           max_features=2000).fit(corpus)
    bag_of_words = vec1.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in
                  vec1.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1],
                reverse=True)
    return words_freq[:n]

top3_words = top_n3_words(comment['ReviewContent'], n=200)
top3_df = pd.DataFrame(top3_words)
top3_df.columns=["Tri-gram", "Freq"]

top3_df

import seaborn as sns
top20_tri_gram = top3_df.iloc[0:20,:]
fig = plt.figure(figsize = (10, 5))
plot=sns.barplot(x=top20_tri_gram["Tri-gram"],y=top20_tri_gram["Freq"])
plot.set_xticklabels(rotation=90,labels = top20_tri_gram["Tri-gram"])
plt.title("Most three words used together in review", y=1.05, fontsize=17);



"""#### WordCloud for entire corpus"""

str_Total = " ".join(comment['ReviewContent'])

fig = plt.figure(figsize = (15,7))
from wordcloud import WordCloud
wordcloud_stw = WordCloud(
                background_color= 'black',
                width = 1800,
                height = 1500, random_state = 10, max_font_size = 200
                ).generate(str_Total)
plt.imshow(wordcloud_stw)
plt.title("WordCloud of melatonion", y=1.07, fontsize=20);

"""Users expectations:
1. How long does it take to fall asleep?(fall asleep faster / quickly)
2. Will I wake up in the middle of the night?(stay asleep / wake middle night / waking middle night)
3. They care about side effect and is melatonin made of natrual ingredients
4. They do care about does it taste good
5. They talk about which dosage works better

#### Sentimental Analysis
"""

from textblob import TextBlob

def get_subjectivity(text):
  return TextBlob(text).sentiment.subjectivity

def get_polarity(text):
  return TextBlob(text).sentiment.polarity

comment['subjectivity'] = comment['ReviewContent'].apply(get_subjectivity)
comment['polarity'] = comment['ReviewContent'].apply(get_polarity)

def getanalysis(score):
  if score < 0:
    return 'Negative'
  elif score == 0:
    return 'Neutral'
  elif score > 0:
      return 'Positive'

comment['Analysis'] = comment['polarity'].apply(getanalysis)
comment['Analysis']

positive_comment = comment[comment['Analysis'] == 'Positive']
positive_comment['ReviewContent'].head()

negative_comment = comment[comment['Analysis'] == 'Negative']
positive_comment['ReviewContent'].head()

plt.figure(figsize=(8,6))
plt.scatter(comment['polarity'], comment['subjectivity'], color='green', alpha=0.2)
plt.title("Sentiment Analysis")
plt.xlabel("Polarity")
plt.ylabel("Subjectivity")
plt.show()

"""sentiment vs dosage"""

bins = [0, 1,3,5, 10, 12, 20]
labels = [
    '0–1mg',
    '1–3mg',
    '3–5mg',
    '5–10mg',
    '10–12mg',
    '12–20mg'
]
comment['DoseGroup'] = pd.cut(comment['Dosage_mg'], bins=bins, labels=labels)

dose_groups = comment['DoseGroup'].unique()

fig, axes = plt.subplots(1, len(dose_groups), figsize=(6 * len(dose_groups), 5), sharex=True, sharey=True)

for ax, group in zip(axes, dose_groups):
    group_df = comment[comment['DoseGroup'] == group]

    ax.scatter(
        group_df['polarity'],
        group_df['subjectivity'],
        alpha=0.25,
        color='green'
    )

    ax.set_title(f"DoseGroup: {group}")
    ax.set_xlabel("Polarity")
    ax.set_ylabel("Subjectivity")

plt.tight_layout()
plt.show()

"""User sentiment distribution almost same across all dosage groups.
User reviews of melatonin are not dose-driven.

sentiment vs brand
"""

brand_groups = comment['Brand'].unique()

fig, axes = plt.subplots(1, len(brand_groups), figsize=(6 * len(brand_groups), 5), sharex=True, sharey=True)

for ax, group in zip(axes, brand_groups):
    group_df = comment[comment['Brand'] == group]

    ax.scatter(
        group_df['polarity'],
        group_df['subjectivity'],
        alpha=0.25,
        color='green'
    )

    ax.set_title(f"BrandGroup: {group}")
    ax.set_xlabel("Polarity")
    ax.set_ylabel("Subjectivity")

plt.tight_layout()
plt.show()



flavor_groups = comment['Flavor'].unique()

fig, axes = plt.subplots(1, len(flavor_groups), figsize=(6 * len(flavor_groups), 5), sharex=True, sharey=True)

for ax, group in zip(axes, flavor_groups):
    group_df = comment[comment['Flavor'] == group]

    ax.scatter(
        group_df['polarity'],
        group_df['subjectivity'],
        alpha=0.25,
        color='green'
    )

    ax.set_title(f"BrandGroup: {group}")
    ax.set_xlabel("Polarity")
    ax.set_ylabel("Subjectivity")

plt.tight_layout()
plt.show()



form_groups = comment['Form'].unique()

fig, axes = plt.subplots(1, len(form_groups), figsize=(6 * len(form_groups), 5), sharex=True, sharey=True)

for ax, group in zip(axes, form_groups):
    group_df = comment[comment['Form'] == group]

    ax.scatter(
        group_df['polarity'],
        group_df['subjectivity'],
        alpha=0.25,
        color='green'
    )

    ax.set_title(f"BrandGroup: {group}")
    ax.set_xlabel("Polarity")
    ax.set_ylabel("Subjectivity")

plt.tight_layout()
plt.show()

"""User sentiment distribution almost same across all brand groups."""



"""#### what are people expacting for buying different dosage? what they praise? complain?

group by dosage

spaCy
"""

import spacy
from spacy.matcher import PhraseMatcher

nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

"""topic_phrases: possible expectations from word freq"""

topic_phrases = {

    "fall_asleep_speed": [
        "fall asleep fast", "fall asleep faster", "fell asleep fast",
        "asleep quickly", "asleep fast", "asleep within minutes",
        "fast acting", "worked quickly", "works quickly",
        "helps me fall asleep", "helped me fall asleep",
        "put me to sleep", "drift off quickly", "easy to fall asleep",
        "30 minutes", "within 30 minutes",
        "hard time falling asleep", "hard time asleep",
        "takes a while to fall asleep", "takes forever",
        "didn't help me fall asleep", "did not help me fall asleep",
        "trouble falling asleep", "still can't fall asleep",
        "could not fall asleep"
    ],

    "stay_asleep?": [
        "stay asleep", "stays asleep", "sleep through the night",
        "slept through the night", "through the night", "all night",
        "entire night", "sleep all night",
        "wake up in the middle of the night", "wake up at night",
        "waking up at night", "woke up at night",
        "waking up during the night", "wake up again",
        "wake multiple times", "kept waking up",
        "middle of the night", "middle night wake"
    ],


    "side_effects": [
        "groggy", "grogginess", "morning groggy",
        "hangover", "hungover",
        "nightmare", "nightmares", "vivid dreams",
        "weird dreams", "strange dreams",
        "headache", "headaches",
        "dizzy", "dizziness", "lightheaded",
        "nausea", "nauseous", "sick to stomach",
        "heart racing", "palpitations",
        "dry mouth", "dry throat",
        "upset stomach", "stomach ache",
        "felt weird", "feel weird",
        "next day groggy",
        "aftertaste", "bad aftertaste"
        "no side effects", "no groggy", "not groggy",
        "no headache", "no dizziness", "no nausea",
        "no hangover", "doesn't make me groggy",
        "didn't feel weird", "no bad dreams",
        "didn't give me nightmares"
    ],


    "effective?": [
        "works great", "works well", "works perfectly",
        "really works", "super effective",
        "helped me sleep", "helps me sleep",
        "effective", "very effective",
        "works amazing", "works wonders",
        "didn't work", "doesn't work", "did not work",
        "not effective", "not very effective",
        "no effect", "no noticeable effect",
        "didn't help", "does not help", "did not help",
        "stopped working", "stop working"
    ],



    "taste": [
        "taste good", "tastes good", "good taste",
        "great taste", "flavor is good", "good flavor",
        "berry flavor", "strawberry", "cherry", "orange",
        "natural berry", "sweet", "tastes great",
        "taste bad", "tastes bad", "bad taste",
        "flavor is bad", "bitter", "chalky",
        "too sweet", "not sweet", "bad flavor",
        "aftertaste", "strong aftertaste"
    ],
}

"""PhraseMatcher"""

matcher = PhraseMatcher(nlp.vocab, attr="LOWER")

for label, phrases in topic_phrases.items():
    patterns = [nlp.make_doc(p) for p in phrases]
    matcher.add(label, patterns)

"""spaCy"""

texts = comment['ReviewContent'].copy().fillna("").astype(str).tolist()
docs = list(nlp.pipe(texts, batch_size=1000))

"""Group by dosage and make a proportional table"""

topic_flags = {label: [] for label in topic_phrases.keys()}

for doc in docs:
    matches = matcher(doc)
    labels_in_doc = {nlp.vocab[match_id].text for match_id, start, end in matches}
    for label in topic_phrases.keys():
        topic_flags[label].append(1 if label in labels_in_doc else 0)

for label, values in topic_flags.items():
    comment[label] = values

topic_cols = list(topic_phrases.keys())

dose_topic_rate = (
    comment
    .groupby('DoseGroup')[topic_cols]
    .mean()
    .reset_index()
)

print(dose_topic_rate)

topic_cols = [
    'fall_asleep_speed','stay_asleep?','side_effects','effective?','taste'
]

plt.figure(figsize=(14,6))
sns.heatmap(
    dose_topic_rate.set_index('DoseGroup')[topic_cols],
    annot=True, cmap='YlGnBu', fmt=".2f"
)
plt.title("Topic Frequency by Dosage Group", fontsize=16)
plt.show()

labels = topic_cols
fig, axes = plt.subplots(2, 3, figsize=(18, 12), subplot_kw=dict(polar=True))
axes = axes.flatten()
dose_groups = dose_topic_rate['DoseGroup'].astype(str).tolist()
for i, dose in enumerate(dose_groups):
    ax = axes[i]
    values = dose_topic_rate[dose_topic_rate['DoseGroup'] == dose][topic_cols].values.flatten()

    values = np.concatenate((values, [values[0]]))
    angles = np.linspace(0, 2 * np.pi, len(topic_cols), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))

    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)

    ax.set_thetagrids(angles[:-1] * 180 / np.pi, topic_cols, fontsize=8)
    ax.set_title(f"Topic Radar – {dose}", fontsize=14)

plt.suptitle("Topic Distribution Across Dosage Groups", fontsize=18)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""conclusion:
1. for different dosage, people are expecting different factors
2. and most of them care about how product taste and side effect
"""



"""postive"""

topic_phrases = {

    "fall_asleep_fast": [
        "fall asleep fast", "fall asleep faster", "fell asleep fast",
        "asleep quickly", "asleep fast", "asleep within minutes",
        "fast acting", "worked quickly", "works quickly",
        "helps me fall asleep", "helped me fall asleep",
        "put me to sleep", "drift off quickly", "easy to fall asleep",
        "30 minutes", "within 30 minutes"
    ],

    "stay_asleep": [
        "stay asleep", "stays asleep", "sleep through the night",
        "slept through the night", "through the night", "all night",
        "entire night", "sleep all night"
    ],

    "side_effects_no": [
        "no side effects", "no groggy", "not groggy",
        "no headache", "no dizziness", "no nausea",
        "no hangover", "doesn't make me groggy",
        "didn't feel weird", "no bad dreams",
        "didn't give me nightmares"
    ],

    "effective_yes": [
        "works great", "works well", "works perfectly",
        "really works", "super effective",
        "helped me sleep", "helps me sleep",
        "effective", "very effective",
        "works amazing", "works wonders"
    ],

    "taste_good": [
        "taste good", "tastes good", "good taste",
        "great taste", "flavor is good", "good flavor",
        "berry flavor", "strawberry", "cherry", "orange",
        "natural berry", "sweet", "tastes great"
    ],
}

matcher = PhraseMatcher(nlp.vocab, attr="LOWER")

for label, phrases in topic_phrases.items():
    patterns = [nlp.make_doc(p) for p in phrases]
    matcher.add(label, patterns)

texts = comment['ReviewContent'].copy().fillna("").astype(str).tolist()
docs = list(nlp.pipe(texts, batch_size=1000))

topic_flags = {label: [] for label in topic_phrases.keys()}

for doc in docs:
    matches = matcher(doc)
    labels_in_doc = {nlp.vocab[match_id].text for match_id, start, end in matches}
    for label in topic_phrases.keys():
        topic_flags[label].append(1 if label in labels_in_doc else 0)

for label, values in topic_flags.items():
    comment[label] = values

topic_cols = list(topic_phrases.keys())

dose_topic_rate = (
    comment
    .groupby('DoseGroup')[topic_cols]
    .mean()
    .reset_index()
)

print(dose_topic_rate)

positive_topic_cols = [
    'fall_asleep_fast', 'stay_asleep',
    'side_effects_no', 'effective_yes', 'taste_good'
]

plt.figure(figsize=(12,5))
sns.heatmap(
    dose_topic_rate.set_index('DoseGroup')[positive_topic_cols],
    annot=True, cmap='YlGnBu', fmt=".2f"
)
plt.title("Positive Topic Frequency by Dosage Group")
plt.show()

labels = positive_topic_cols
fig, axes = plt.subplots(2, 3, figsize=(18, 12), subplot_kw=dict(polar=True))
axes = axes.flatten()

dose_groups = dose_topic_rate['DoseGroup'].astype(str).tolist()

for i, dose in enumerate(dose_groups):
    ax = axes[i]
    values = dose_topic_rate.loc[dose_topic_rate['DoseGroup'] == dose, positive_topic_cols].values.flatten()

    values = np.concatenate((values, [values[0]]))
    angles = np.linspace(0, 2 * np.pi, len(positive_topic_cols), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))

    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)

    ax.set_thetagrids(angles[:-1] * 180 / np.pi, labels, fontsize=8)
    ax.set_title(f"Positive Topics – {dose}", fontsize=14)

plt.suptitle("Positive Topic Distribution Across Dosage Groups", fontsize=18)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""conclusion:
1. for different dosage people praise about different effect
2. but most of them praise about taste
"""

topic_phrases = {

    "fall_asleep_slow": [
        "hard time falling asleep", "hard time asleep",
        "takes a while to fall asleep", "takes forever",
        "didn't help me fall asleep", "did not help me fall asleep",
        "trouble falling asleep", "still can't fall asleep",
        "could not fall asleep"
    ],

    "middle_night_wake": [
        "wake up in the middle of the night", "wake up at night",
        "waking up at night", "woke up at night",
        "waking up during the night", "wake up again",
        "wake multiple times", "kept waking up",
        "middle of the night", "middle night wake"
    ],

    "side_effects_yes": [
        "groggy", "grogginess", "morning groggy",
        "hangover", "hungover",
        "nightmare", "nightmares", "vivid dreams",
        "weird dreams", "strange dreams",
        "headache", "headaches",
        "dizzy", "dizziness", "lightheaded",
        "nausea", "nauseous", "sick to stomach",
        "heart racing", "palpitations",
        "dry mouth", "dry throat",
        "upset stomach", "stomach ache",
        "felt weird", "feel weird",
        "next day groggy",
        "aftertaste", "bad aftertaste"
    ],

    "effective_no": [
        "didn't work", "doesn't work", "did not work",
        "not effective", "not very effective",
        "no effect", "no noticeable effect",
        "didn't help", "does not help", "did not help",
        "stopped working", "stop working"
    ],

    "taste_bad": [
        "taste bad", "tastes bad", "bad taste",
        "flavor is bad", "bitter", "chalky",
        "too sweet", "not sweet", "bad flavor",
        "aftertaste", "strong aftertaste"
    ],
}

matcher = PhraseMatcher(nlp.vocab, attr="LOWER")

for label, phrases in topic_phrases.items():
    patterns = [nlp.make_doc(p) for p in phrases]
    matcher.add(label, patterns)

texts = comment['ReviewContent'].copy().fillna("").astype(str).tolist()
docs = list(nlp.pipe(texts, batch_size=1000))

topic_flags = {label: [] for label in topic_phrases.keys()}

for doc in docs:
    matches = matcher(doc)
    labels_in_doc = {nlp.vocab[match_id].text for match_id, start, end in matches}
    for label in topic_phrases.keys():
        topic_flags[label].append(1 if label in labels_in_doc else 0)

for label, values in topic_flags.items():
    comment[label] = values

topic_cols = list(topic_phrases.keys())

dose_topic_rate = (
    comment
    .groupby('DoseGroup')[topic_cols]
    .mean()
    .reset_index()
)

print(dose_topic_rate)

negative_topic_cols = [
    'fall_asleep_slow', 'middle_night_wake',
    'side_effects_yes', 'effective_no', 'taste_bad'
]

labels = negative_topic_cols
fig, axes = plt.subplots(2, 3, figsize=(18, 12), subplot_kw=dict(polar=True))
axes = axes.flatten()

dose_groups = dose_topic_rate['DoseGroup'].astype(str).tolist()

for i, dose in enumerate(dose_groups):
    ax = axes[i]
    values = dose_topic_rate.loc[dose_topic_rate['DoseGroup'] == dose, negative_topic_cols].values.flatten()

    values = np.concatenate((values, [values[0]]))
    angles = np.linspace(0, 2 * np.pi, len(negative_topic_cols), endpoint=False)
    angles = np.concatenate((angles, [angles[0]]))

    ax.plot(angles, values, linewidth=2)
    ax.fill(angles, values, alpha=0.25)

    ax.set_thetagrids(angles[:-1] * 180 / np.pi, labels, fontsize=8)
    ax.set_title(f"Positive Topics – {dose}", fontsize=14)

plt.suptitle("Positive Topic Distribution Across Dosage Groups", fontsize=18)
plt.tight_layout(rect=[0, 0, 1, 0.95])
plt.show()

"""conclusion:
1. for different dosage people complain for different reason
2. most of them complain about side effect
"""



"""Now able to answer:
1. Do different dosages affect market feedback?
   
   dosage does not meaningfully affect consumer feedback(review score and sentiment distribution).
3. Is the feedback influenced more by other factors?

   not the product attributes, but their expectations
5. What do people expect for each dosage?

   they expect: fall asleep fast, no middle wake up, taste good, no side effect, efficiency
7. What are they praise\complain about for each dosage?

   different dosage, they praise\complain about different things.
   but generally they praise about good taste and complain about side effect.
"""



